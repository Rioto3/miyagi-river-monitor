import requests
from bs4 import BeautifulSoup
import datetime
import os
import re
import json
from urllib.parse import urljoin

class MiyagiRiverScraper:
    def __init__(self, base_url='https://www.pref.miyagi.jp/life/4/15/49/index.html'):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
        })
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹JSONãƒ•ã‚¡ã‚¤ãƒ«
        self.metadata_file = 'miyagi_river_metadata.json'
        # é€šçŸ¥ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
        self.output_file = 'notification_output.txt'

    def fetch_page(self):
        """ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¦BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™"""
        try:
            response = self.session.get(self.base_url)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return BeautifulSoup(response.text, 'html.parser')
        except requests.exceptions.RequestException as e:
            print(f"ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return None

    def extract_articles(self, soup):
        """è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹"""
        if not soup:
            return []

        articles = []
        content_div = soup.find('div', id='tmp_contents')
        if not content_div:
            print("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é ˜åŸŸãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return []

        ul_element = content_div.find('ul')
        if not ul_element:
            print("è¨˜äº‹ãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return []

        for li in ul_element.find_all('li'):
            try:
                # ãƒªãƒ³ã‚¯è¦ç´ ã‚’å–å¾—
                link = li.find('a')
                if not link:
                    continue
                
                # ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‚’å–å¾—ã—ã¦è§£æ
                full_text = li.get_text().strip()
                
                # æ—¥ä»˜ã‚’æ­£è¦è¡¨ç¾ã§æŠ½å‡º (YYYYå¹´MMæœˆDDæ—¥ ã®å½¢å¼)
                date_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', full_text)
                if date_match:
                    year = int(date_match.group(1))
                    month = int(date_match.group(2))
                    day = int(date_match.group(3))
                    
                    # æ—¥ä»˜æ–‡å­—åˆ—ã®å½¢å¼
                    date_str = f"{year}å¹´{month}æœˆ{day}æ—¥"
                    
                    # æ•°å€¤å½¢å¼ã®æ—¥ä»˜ï¼ˆYYYYMMDDï¼‰
                    date_value = year * 10000 + month * 100 + day
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã¯ãƒªãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆ
                    title = link.get_text().strip()
                    
                    # URL
                    url = urljoin(self.base_url, link['href'])
                    
                    articles.append({
                        'date_str': date_str,
                        'date_value': date_value,
                        'title': title,
                        'url': url
                    })
                else:
                    print(f"æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {full_text[:30]}...")
            except Exception as e:
                print(f"è¨˜äº‹ã®æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

        # æ—¥ä»˜ã®æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆ
        articles.sort(key=lambda x: x['date_value'], reverse=True)
        return articles
    
    def load_metadata(self):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
        if os.path.exists(self.metadata_file):
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                return self.create_default_metadata()
        return self.create_default_metadata()
    
    def create_default_metadata(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
        now = datetime.datetime.now()
        return {
            "last_date_value": 0,  # æœ€å¾Œã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸè¨˜äº‹ã®æ—¥ä»˜å€¤
            "last_run": now.strftime("%Y-%m-%d %H:%M:%S"),  # æœ€å¾Œã®å®Ÿè¡Œæ—¥æ™‚
            "total_articles_found": 0,  # ã“ã‚Œã¾ã§ã«è¦‹ã¤ã‘ãŸè¨˜äº‹ã®ç·æ•°
            "total_new_articles": 0,  # ã“ã‚Œã¾ã§ã«è¦‹ã¤ã‘ãŸæ–°ã—ã„è¨˜äº‹ã®ç·æ•°
            "source_url": self.base_url  # ã‚½ãƒ¼ã‚¹URL
        }
    
    def save_metadata(self, metadata):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    def update_metadata(self, metadata, articles, new_articles):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹"""
        now = datetime.datetime.now()
        
        # æœ€æ–°ã®è¨˜äº‹æ—¥ä»˜ã‚’æ›´æ–°ï¼ˆè¨˜äº‹ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
        if articles:
            metadata["last_date_value"] = articles[0]["date_value"]
        
        # æœ€çµ‚å®Ÿè¡Œæ—¥æ™‚ã‚’æ›´æ–°
        metadata["last_run"] = now.strftime("%Y-%m-%d %H:%M:%S")
        
        # ç´¯è¨ˆã®è¨˜äº‹æ•°ã‚’æ›´æ–°
        metadata["total_articles_found"] += len(articles)
        metadata["total_new_articles"] += len(new_articles)
        
        return metadata
    
    def find_new_articles(self, articles, last_date_value):
        """å‰å›ã®æœ€æ–°æ—¥ä»˜ã‚ˆã‚Šæ–°ã—ã„è¨˜äº‹ã‚’æŠ½å‡ºã™ã‚‹"""
        return [article for article in articles if article['date_value'] > last_date_value]
    
    def generate_notification_text(self, new_articles):
        """é€šçŸ¥ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¦ä¿å­˜ã™ã‚‹"""
        if not new_articles:
            # æ–°ã—ã„è¨˜äº‹ãŒãªã„å ´åˆã¯ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            with open(self.output_file, 'w', encoding='utf-8') as f:
                f.write("")
            return False
        
        try:
            with open(self.output_file, 'w', encoding='utf-8') as f:
                for i, article in enumerate(new_articles):
                    message = f"ğŸŒŠ å®®åŸçœŒæ²³å·æƒ…å ±ãƒ‹ãƒ¥ãƒ¼ã‚¹é€Ÿå ± ğŸš¨\n\n"
                    message += f"ğŸ“… æ—¥ä»˜: {article['date_str']}\n"
                    message += f"ğŸ“° ã‚¿ã‚¤ãƒˆãƒ«: {article['title']}\n"
                    message += f"ğŸ”— ãƒªãƒ³ã‚¯: {article['url']}"
                    
                    # æœ€å¾Œã®è¨˜äº‹ã§ãªã‘ã‚Œã°åŒºåˆ‡ã‚Šç·šã‚’è¿½åŠ 
                    if i < len(new_articles) - 1:
                        message += "\n\n-------------------\n\n"
                    
                    f.write(message)
            
            print(f"{len(new_articles)}ä»¶ã®é€šçŸ¥ã‚’ {self.output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")
            return True
        except Exception as e:
            print(f"é€šçŸ¥ãƒ†ã‚­ã‚¹ãƒˆã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return False

    def run(self):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹"""
        print(f"å®®åŸçœŒæ²³å·æƒ…å ±ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
        metadata = self.load_metadata()
        last_date_value = metadata.get("last_date_value", 0)
        print(f"å‰å›ã®æœ€æ–°è¨˜äº‹æ—¥ä»˜: {last_date_value}")
        
        # ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        soup = self.fetch_page()
        articles = self.extract_articles(soup)
        
        has_new_articles = False
        
        if articles:
            print(f"{len(articles)}ä»¶ã®è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            
            # æœ€æ–°ã®è¨˜äº‹æ—¥ä»˜ã‚’å–å¾—ï¼ˆã‚½ãƒ¼ãƒˆæ¸ˆã¿ãªã®ã§å…ˆé ­ã®è¨˜äº‹ï¼‰
            latest_date = articles[0]['date_value']
            print(f"ç¾åœ¨ã®æœ€æ–°è¨˜äº‹æ—¥ä»˜: {latest_date}")
            
            # æ–°ã—ã„è¨˜äº‹ã‚’è¦‹ã¤ã‘ã‚‹
            new_articles = self.find_new_articles(articles, last_date_value)
            print(f"ãã®ã†ã¡{len(new_articles)}ä»¶ãŒæ–°ã—ã„è¨˜äº‹ã§ã™")
            
            # æ–°ã—ã„è¨˜äº‹ãŒã‚ã‚Œã°é€šçŸ¥ç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
            if new_articles:
                has_new_articles = self.generate_notification_text(new_articles)
            else:
                # æ–°ã—ã„è¨˜äº‹ãŒãªã„å ´åˆã¯ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
                open(self.output_file, 'w').close()
                print("æ–°ã—ã„è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã¦ä¿å­˜
            updated_metadata = self.update_metadata(metadata, articles, new_articles)
            self.save_metadata(updated_metadata)
            
            return has_new_articles
        else:
            print("è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            # è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚‰ãªãã¦ã‚‚ã€å®Ÿè¡Œè¨˜éŒ²ã¯æ®‹ã™
            updated_metadata = self.update_metadata(metadata, [], [])
            self.save_metadata(updated_metadata)
            # ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            open(self.output_file, 'w').close()
            return False

if __name__ == "__main__":
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’åˆæœŸåŒ–ã—ã¦å®Ÿè¡Œ
    scraper = MiyagiRiverScraper()
    has_new_articles = scraper.run()
    
    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®çµ‚äº†ã‚³ãƒ¼ãƒ‰ã§æ–°ã—ã„è¨˜äº‹ãŒã‚ã‚‹ã‹ã©ã†ã‹ã‚’è¿”ã™
    # GitHubãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ä½¿ç”¨å¯èƒ½
    exit(0 if has_new_articles else 1)